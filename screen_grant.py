EXAMPLE_GRANT_DESCRIPTION = """
# **Summary**\n\nAI safety has a knowledge transfer challenge. As the field grows, each new entrant trying to understand safety problems needs to invest significant time understanding core concepts before they can contribute meaningfully. This creates a bottleneck of "missing interpretive labor" - the effort required to understand ideas that could otherwise go toward advancing safety strategies. If each new entrant has to independently spend hours piecing together the same understanding, we waste a lot of valuable impact time. As capabilities accelerate, this bottleneck of missing interpretive labor becomes increasingly costly.\n\nThe Atlas Project addresses this challenge by creating systematic explanations of AI safety concepts. We create a shared foundation of understanding, accelerating the integration of new contributors, enabling more effective governance discussions, and broadening engagement by clearly articulating both immediate and long-term challenges in AI development. Rather than each person independently struggling through scattered explanations, we craft clear, progressive learning pathways that build understanding step by step. The impact is already evident: in just six months, over 400 students across multiple institutions have adopted our materials, with course organizers consistently returning to them across multiple semesters.\n\nEvery hour saved in understanding multiplies across the community. When a clear explanation saves each person just one hour, the impact potentially scales to collectively hundreds of hours redirected toward meaningful engagement with AI safety. With funding, we can significantly improve our impact by improving existing materials, creating supporting resources in multiple formats (like audio/video) and languages to reach even more audiences.\n\n[Here is a link](https://ai-safety-atlas.com/chapters/) to the current version of the Atlas.\n\n# **Project Vision & Value Proposition**\n\nTL;DR\n\n* AI safety faces a knowledge transfer bottleneck as the field rapidly expands\n* Full time work on creating clear explanations can save potentially hundreds of hours on coordination and understanding\n* Recent developments increase urgency of efficient knowledge transfer\n* High-quality distillation requires systematic, dedicated full time effort currently missing in the field\n\n**What is interpretive labor and why does it matter?** There\'s a fundamental tradeoff between the energy put into explaining an idea and the energy needed to understand it. At one extreme, someone can craft a beautiful explanation that makes understanding feel effortless. At the other, they can do the minimum and leave their audience to struggle. This balance of energy is called [interpretive labor](https://acesounderglass.com/2015/06/09/interpretive-labor/). ([Olah & Carter, 2017](https://distill.pub/2017/research-debt/))\n\nThe core vision of the Atlas project is to accelerate AI safety progress by performing interpretive labor and reducing research debt. Our work serves three critical functions. First, it accelerates the technical researcher pipeline by helping new researchers quickly build deep understanding. Second, it enables more effective governance by helping policymakers engage with technical concepts. Third, it moves the Overton window by clearly articulating both immediate and long-term risks, making AI safety more accessible to broader audiences who influence the trajectory of AI development.\n\nWhat makes our approach uniquely valuable is that the Atlas provides one of the only comprehensive end-to-end explanations of AI safety available today. Unlike distillation resources that focus on single agendas or isolated concepts, our 600+ page literature review creates a complete conceptual map—from fundamental risks and capabilities through governance frameworks to technical safety strategies and their limitations. This comprehensive approach allows readers to understand how all pieces of AI safety research fit together. We trace the connections between problems and potential solutions, showing both what we know and the critical gaps where more work is needed. By integrating perspectives from hundreds of sources across academia, industry labs, and independent research, we provide the essential context that allows newcomers to grasp not just individual concepts, but the full landscape of AI safety efforts. This holistic understanding enables more effective coordination and reduces redundant work across the field.\n\n**Why do multiplier effects make this more impactful?** The cost of explaining stays constant whether reaching one person or thousands, but the cost of understanding multiplies with each new person entering the field. Understanding AI safety requires integrating knowledge from an overwhelming number of domains - from machine learning and cognitive science to philosophy of mind, game theory, governance frameworks, and many more. When understanding becomes too difficult, people either give up or specialize prematurely, missing important connections. Research debt accumulates, ideas get forgotten or repeatedly rediscovered, and the field fragments. ([Olah & Carter, 2017](https://distill.pub/2017/research-debt/))\n\n**Why do we think this vision needs more support?** Like the theoretician or research engineer, the research distiller is an integral role for a healthy research community. But right now, almost no one fills it. Many want to work on research distillation but lack the support structure - there\'s no clear career path, few places to learn, and limited examples to follow. The field often doesn\'t recognize distillation as a real research contribution, overlooking how crucial systematic knowledge transfer is for scaling impact in AI safety.\n\nAI CEOs like Altman and Amodei estimate transformative AI within 3 years ([LintzA, 2025](https://www.lesswrong.com/posts/ynsjJWTAMhTogLHm6/the-game-board-has-been-flipped-now-is-a-good-time-to)). Even accounting for potential bias in this estimate, at the very least we can say that recent developments like DeepSeek r1, OpenAI o3 and [Stargate](https://openai.com/index/announcing-the-stargate-project/) show the acceleration of capabilities. EpochAI estimates progress to keep speeding up ([EpochAI, 2025](https://epochai.substack.com/p/ai-progress-is-about-to-speed-up)). Technical understanding that previously could be built over years now needs to be acquired in months. Without systematic knowledge transfer, we risk critical safety work being bottlenecked by the time it takes each person to piece together understanding.\n\nFrom our perspective, creating these explanations isn\'t just adding a small layer of polish to existing research - it requires transforming ideas completely. To distill an idea while also connecting it to others, we need to have a deep technical understanding plus the ability to craft intuitive progressions that make complex concepts click. This is why we need dedicated, full-time focus. Every hour we invest in this work saves hundreds of collective hours down the line.\n\n# **Team**\n\n**Markov** is the primary author and project developer. He contributes across all aspects of the project - research, writing, distillation, website development, video creation, etc. He was previously a scriptwriter for AI safety videos at [Rational Animations](https://www.youtube.com/@RationalAnimations/videos), worked as a distillation fellow at Rob Miles’ [AI Safety Info (Stampy)](https://stampy.ai/) project, where he wrote explanations of dozens of AI safety concepts, and also contributed to also building the retrieval augmented generation (RAG) based [distillation chatbot](https://www.lesswrong.com/posts/bT8yyJHpK64v3nh2N/ai-safety-chatbot). He is also co-founder and CTO at [Equilibria Network](https://eq-network.org/), and has previously worked in both software development and cyber security.\n\n**Charbel** is the Executive Director of [CeSIA](https://www.securite-ia.fr/en). He leads organization, scientific direction and coordination. He brings significant pedagogical experience, including supervising ARENA projects, MLAB, and developing Europe\'s first general purpose AI safety course. As a quick example, his writing is part of [BlueDot\'s official interpretability curriculum](https://course.aisafetyfundamentals.com/alignment?session=6). His posts analyzing and [distilling Davidad’s research](https://www.lesswrong.com/posts/jRf4WENQnhssCb6mJ/davidad-s-bold-plan-for-alignment-an-in-depth-explanation) received significant traction, and two of his posts were also counted amongst the best of LessWrong 2023.\n\nWe also have **Jeanne Salle** (AI safety teacher at ENS Ulm), and **Charles Martinet** (head of policy at CeSIA, research affiliate at Oxford AIGi and previously at GovAI) as contributing authors.\n\n[**Vincent Corruble**](https://scholar.google.com/citations?user=cZZriocAAAAJ&hl=fr) is a project advisor. He is a professor at Sorbonne University and a research fellow at CHAI.\n\n[**Fabien Roger**](https://scholar.google.com/citations?user=La75jqEAAAAJ&hl=en) is scientific advisor. He has worked at Redwood Research, and is now at Anthropic.\n\nIf you want more detailed track records, please reach out privately and we can send over CVs.\n\n# **Track Record & Impact Analysis**\n\n**TL;DR:**\n\n* Created comprehensive AI safety literature review used by 400+ students\n* Built efficient systems for ongoing content creation and updates\n* Achieved organic adoption across multiple institutions\n* Received specific, actionable feedback for improvements to existing content\n* Developed scalable AI research distillation pipeline\n\n**What have we built so far?** In the last few months, we\'ve developed the [AI Safety Atlas](https://ai-safety-atlas.com/chapters/) \\- a systematic treatment and literature review of most concepts in AI safety. The text covers everything from fundamental concepts like capabilities and risks through to technical topics like interpretability and oversight, split across nine chapters. Besides just writing the core text, we have [integrated interactive elements](https://ai-safety-atlas.com/chapters/01/04/#02), [developed low friction collaboration mechanisms](https://docs.google.com/document/d/1KI95w27Ce7yWoynE11PJ94IXK0gT0NwP8091s06P7wM/edit?tab=t.0#heading=h.samhjn8xpy3h) (open source editing → website → LaTeX automated pipeline), experimented with [audio/video content](https://www.youtube.com/watch?v=dhr4u-w75aQ), and built sustainable systems to maintain and improve it alongside new developments in AI.\n\n**How is what we built being used?** Several groups have integrated the Atlas into their curricula. ML4Good uses our materials as core reading content, reaching 250 students annually. ENS Ulm and Saclay have built their university courses around our text, serving \\~100 students per year. The AI Safety Collab (\\~80 students) and UBC Vancouver (30-50 students) used our materials in their programs and decided to use it again for their new iteration. We have also been cold approached by many individual independent readers thanking us for our work. The European Network for AI Safety (ENAIS) [has also shown interest in using our work](https://docs.google.com/document/d/1t4DAivNipzP0-L6uytGcUCi-wEi3YrHiH0t1XOxC6Ws/edit?tab=t.0) as part of various courses, and they think CeSIA is in a unique position to do this kind of work.\n\n**What have we learned from the initial impact?** Two aspects of our growth stand out. First, adoption has been largely organic - we had very limited resources, so we didn\'t spend on any marketing or outreach. Course organizers are discovering our materials and choosing to use them based on merit. Second, we see strong repeat usage. When institutions try our materials, they tend to stick with them. Both of these - largely organic growth combined with repeated use across multiple semesters is quite encouraging.\n\n**How do we measure quality and impact?** We track detailed metrics for every chapter including overall ratings, writing clarity, self-reported understanding, section-specific ratings, and categorized improvement requests. Newer chapters (averaging 4.2/5) significantly outperform older chapters (3.7/5). Our success metric is achieving 4.5/5 on understanding and recommendation likelihood.\n\nAs for evaluating the quality of our current materials, here is a quote from an AI Safety organizer at UBC Vancouver (Jan 2025): "_For a systematic, centralized, and concise introduction to the core topics in AI safety, this is the best and most up-to-date resource I know. That\'s why we chose it as the basis for our Intro to AI Alignment course._"\n\nAnd a quote from a student (Feb 2025): "_This was my first time reading the AI Safety Atlas and wow!!! It has so much insightful information and citations that I am looking forward to diving deeper into. I am super fascinated by this text and I feel I have learned so much from it already._"\n\n**How are we evolving our approach based on what we\'ve learned?** Our initial approach to content development faced two challenges: First, connections between concepts helped students most, yet our early chapters lacked enough of these connections. Second, keeping up with rapidly evolving AI safety literature became increasingly difficult. To address these challenges, we\'ve started developing an AI-augmented distillation pipeline for our newest content. This analyzes hundreds of sources simultaneously across:\n\n* Research papers (ArXiv, PubMed, Nature, IEEE...)\n* Blog posts (Alignment Forum, LessWrong, MIRI...)\n* Podcast transcripts (AXRP, MLST, Inside View...)\n* Research reports & Think Tanks (RAND, CSET, IAPS, GovAI...)\n* Other course materials (MATS, CAIS, BlueDot...)\n\nWe\'ve only applied this new methodology to our most recent chapter (Evaluations) and parts of the Risks chapter, but we\'ve already seen improved student feedback: "_I like the amount of real-life examples incorporated into the writing when talking about a certain concept, and the citations following those examples so we can go more in depth into specific examples if we like_."\n\n**The growth trajectory looks good and is expected to scale further with more outreach.** We know which parts need improvement. We have granular feedback telling us which specific sections in each chapter are strong/weak, and what exactly the students feel is lacking. Students want more conceptual connections across concepts, more comprehensive coverage and visual explanations to aid the text. Facilitators need better supporting materials to run effective discussions.\n\nOur more recent chapters, written with improved experience, meet a higher quality standard than the ones written at the start of the project. We now have the processes, expertise, and feedback mechanisms to systematically upgrade all content to our current higher standards. We just need more time and resources to continue implementing these improvements at scale while expanding into new formats that enhance understanding.\n\n# **Ecosystem Positioning**\n\nDifferent programs serve different needs. We aim to support and complement existing approaches by creating base layer explanations that they can further build their programs and courses upon.\n\n[BlueDot](https://aisafetyfundamentals.com/) does a great job at carefully curating the best existing explanations to create learning experiences instead of writing new explanations. The more focused programs like [MATS](https://www.matsprogram.org/), [ARENA](https://www.arena.education/), [SPAR](https://sparai.org/), [ERA](https://erafellowship.org/), … focus specifically on mentorship, hands-on engineering and research development.\n\nAs for independent course organizers, they can use whatever materials work best for their specific needs. Research fellowship and mentorship programs can use our materials to establish foundations before moving to specialized training. We actively try to see what knowledge requirements/learning outcomes such programs have (e.g. [DeepMind Safety education materials](https://deepmindsafetyresearch.medium.com/introducing-our-short-course-on-agi-safety-1072adb7912c), [MATS curriculum](https://www.lesswrong.com/posts/rhEXTkDmssrHBNrfS/mats-ai-safety-strategy-curriculum-v2), [Cooperative AI curriculum](https://www.cooperativeai.com/), [BlueDot’s](https://aisafetyfundamentals.com/economics-of-tai/) [various](https://aisafetyfundamentals.com/alignment/) [curricula](https://aisafetyfundamentals.com/governance/), and more …) and incorporate that into our work. We\'re not trying to replace these approaches - we\'re trying to make existing ones more effective by reducing the effort needed to understand core concepts.\n\nOur work shares general goals with the [CAIS textbook](https://www.aisafetybook.com/) in making AI safety concepts accessible, but we offer distinct and complementary coverage. The CAIS textbook is very good at providing a systems-oriented foundation and focuses a large chunk of its explanations on topics like safety engineering, collective action problems and complex systems theory. The Atlas takes a slightly different approach by going deeper into specific problems and mitigation strategies.\n\nAs concrete comparisons, the CAIS textbook condenses topics like evaluations, monitoring, and interpretability into [a single subsection](https://www.aisafetybook.com/textbook/monitoring#mechanistic-interpretability). The Atlas dedicates [entire](https://ai-safety-atlas.com/chapters/05/) [chapters](https://ai-safety-atlas.com/chapters/09/) to each of these domains individually, providing much greater technical depth. Similarly, while CAIS addresses [alignment](https://www.aisafetybook.com/textbook/alignment) and [robustness](https://www.aisafetybook.com/textbook/robustness#proxies-in-machine-learning) in one subsection, we offer multiple comprehensive chapters on specific risks, and alignment challenges: [scalable oversight](https://ai-safety-atlas.com/chapters/08/), [misspecification](https://ai-safety-atlas.com/chapters/06/) and [misgeneralization](https://ai-safety-atlas.com/chapters/07/) \\- each exploring both problem, risks and current solution strategies. We also have potential future planned chapters on cybersecurity, systems theory and multi agent alignment research.\n\nThese structural differences illustrate how both works serve complementary purposes in the ecosystem. Collectively, alongside BlueDot, CAIS, and all the other field building programs, we can strengthen the field by reaching different learning styles and serving various educational needs.\n\n# **Plans & Deliverables**\n\nTL;DR:\n\n1. Six core deliverables: quality updates to existing chapters, professional translations, audio versions, video explanations, teaching support, and technical improvements (website, parser, research scraper, etc.)\n2. Each chapter upgrade represents substantial enhancement based on our new methodology\n3. Clear metrics track improvements over time, with a target success metric of 4.5/5 rating for self reported understanding  \n   * Other metrics that will inform our path and progress include number of students, and engagement on audio/video.\n4. Supporting materials address specific needs identified by students and facilitators\n5. Flexibility to adjust priorities based on user feedback and demonstrated impact\n\nOur future plans focus on six key deliverables that address different aspects of knowledge transfer in AI safety. The scope and timeline for delivering these improvements depends directly on funding level.\n\n**Deliverable 1: Quality updates to existing chapters.** All the existing chapters are functional, but we want to raise the bar for quality. Updates are quite substantial, they don\'t mean simple edits or maintenance. We highlighted in the track record section that we have grown as research communicators, and have also developed better methodologies for better distillations and wider literature review.\n\nTo be a little more concrete on what an update to an existing chapter would look like, let\'s use the example of the Generalization chapter. Currently it adequately explains concepts like goal misgeneralization and deceptive alignment, but it draws mainly from a few DeepMind and MIRI papers. An updated version would integrate much broader ML literature on inductive biases, geometry of loss landscapes, and out-of-distribution generalization - showing how different training approaches lead to different learned algorithms. It would incorporate more empirical demonstrations, disambiguate concepts like situational awareness and scheming, and strengthen connections to other chapters. Each section upgrade requires at least 20 hours (each chapter is made up of 3-5 sections), covering feedback integration, comprehensive literature review, original diagrams, and cross-chapter connections. \n\n**Improving our content and developing our technical pipeline are pretty interrelated processes.** In order to create better explanations, we also simultaneously work on refining the AI-augmented distillation tools that support this work. This is mainly for scalability and sustainability reasons, since we can\'t keep up with the amount of new research being published without AI augmentation. At higher funding tiers, we\'ll invest more in extending this pipeline, eventually potentially creating a fork of the [alignment research dataset (ARD)](https://huggingface.co/datasets/StampyAI/alignment-research-dataset/blob/main/README.md) and building upon existing work. This means funding doesn\'t just support better content today but builds lasting infrastructure for scaling knowledge transfer in AI safety.\n\n**Deliverable 2: Teaching materials & facilitator support.** [A retrospective by facilitators from 13 AI safety university groups in 2024](https://www.lesswrong.com/posts/EsFbCyeA2uLxC4jMe/ai-safety-university-organizing-early-takeaways-from) highlighted various needs that we want to support. As one example, they specifically requested PDFs and printed materials (shown to increase retention) and discussion guides. We\'ve already started to build a parser pipeline that renders content into multiple formats (including LaTeX) but this needs a little more development time to be seamlessly integrated and ensure consistent quality. We also want to provide printed copies for various universities to use based on this request. This is one example of facilitator support which is not just restricted to building facilitation guides or coordination. It also highlights why at times we may need help scaling with software development contractors.\n\n**Deliverable 3: Audio content.** We\'ve tested AI-assisted audio generation (NotebookLM) but still need human review to ensure accuracy. With sufficient funding, we could produce accurate higher quality manual recordings.\n\n**Deliverable 4: Video explanations.** We\'ve also experimented with some basic pilot video content at Sorbonne University with positive feedback. Our intuition is that having videos could be very impactful. So we want to experiment by providing more video content for our chapters. Videos require approximately 40 hours each for script development, diagram creation, recording, and post-production. This is in addition to the time required to update the core writing for the chapter.\n\n**Deliverable 5: Website improvements.** As our primary engagement medium, the website needs improvements including more interactive elements, professional design, and reliable content rendering. Previous UI/UX improvements have shown strong returns on investment through increased engagement. We believe there is still a lot of room to improve the website for both functionality and aesthetics, so we want to dedicate time and effort into doing so.\n\n**Deliverable 6: Professional translations.** We want to deepen integration with the international AI Safety ecosystem, particularly Chinese. We can use AI assisted tools for basic translations but this does not always mean high quality. Mandarin would be the highest priority, but we also have volunteers for some other languages (French, German, Portuguese). Funding would allow fair compensation for work, and allows us to ensure professional-quality translations while maintaining technical accuracy.\n\nWe maintain flexibility to adjust priorities based on demonstrated impact - redirecting resources if certain formats show exceptional engagement or value.\n\n# **Funding & cost breakdowns**\n\n**TL;DR:**\n\n* Demonstrated impact with minimal resources (400+ students across multiple institutions)\n* Currently constrained by having just one full-time team member\n* Four funding tiers designed for progressive impact scaling:  \n   * €47k/6mo: Maintains one FTE focused on updates to 4 priority chapters, develops at least 1 pilot video.  \n   * €94k/12mo: Updates all 9 chapters, creates new Cooperative AI chapter, produces 4+ videos, upgrades facilitator guides.  \n   * €134k/12mo: Adds funding for contractor support for a Chinese translation, a paid facilitator program (one round of multiple cohorts), and paid development work.  \n   * €183k/12mo: Extends funding to 5 professional translations, 3 facilitation cohorts, advanced technical development of a scalable automated distillation assistant.\n\nOur initial six months was funded by a [Manifund regrant by Ryan Kidd](https://manifund.org/projects/ai-safety-textbook) and Open Philanthropy. This bootstrap funding helped create a MVP, and demonstrate clear demand and impact. Given the fact that the content created from the initial grant is now used by hundreds of students across multiple institutions, we believe we fulfilled that objective. However, we\'re currently severely constrained by having just one full-time team member handling everything from content development to technical infrastructure. We need funding to maintain and improve our core explanatory content, expand into multimedia formats that enhance understanding, and ideally scale our impact through translations and comprehensive facilitator support.\n\nWe have thought a lot about this detailed funding breakdown into granular amounts, hourly rates, and deliverables at [this link](https://docs.google.com/document/d/1xppKdR8x4zwCblyMnHErOn6I-8LhS5CdWHkU6CwejCA/edit?usp=sharing). This document is only presenting an overview.\n\n## **€46,300/6 months (Minimal Scenario)**\n\nThis minimal funding maintains project continuity and enables focused improvement of some highest-priority chapters. It retains one full-time developer to continue basic operations and produce three core deliverables: quality updates to key chapters, initial video development, and ongoing website improvements. This tier represents bare minimum viability - enough to keep the project alive and demonstrate our improved methodology, but far from what we believe is the project\'s full potential.\n\n**Time Allocation & Deliverables:**\n\n* Core Content Updates  \n   * Significant improvements to four chapters: Strategies, Specification, Generalization, Interpretability\n* Video Content Development  \n   * Create at least one pilot video  \n   * Record lectures and presentations (minimal post-production)\n* Technical Infrastructure  \n   * Parser development + maintenance  \n   * Website transition to new framework  \n   * Continue development on distillation pipeline - scrapers, aggregators, etc.\n\n**Recurring Costs: €6,900/month (€41,400 total, 89.4%)**\n\n**One-time Costs: €4,900 (10.6%)**\n\n## **€94,000/12 months (Extended Minimal Scenario)**\n\nA full year of funding allows us to systematically improve all existing content while developing essential supporting materials for educators. This longer runway enables us to implement our research distillation methodology across the entire textbook, write new chapters on missing topics, and create teaching support materials. While this provides a more sustainable approach, we still rely on one full-time contributor and volunteer work for several core deliverables - facilitation, translations, and audio.\n\n**Time Allocation & Deliverables (in addition to all previous)**\n\n* Core Content Updates  \n   * Significantly improve all 9 existing chapters (vs. only 4 chapters)  \n   * Write new chapter: Cooperative AI/Multi-agent alignment\n* Video Content Development  \n   * Create 4+ educational videos  \n   * Record lectures and presentations (minimal post-production)\n* Facilitator Support Materials  \n   * Further develop discussion guides  \n   * Further develop consolidated assessment tools and feedback forms for proper progress tracking\n\n**Recurring Costs (88.1%): €6.9k/month (€82.8k total)**\n\n**One-time Costs (11.9%): €11.2k total**\n\n## **€133,500/12 months (Moderate Scenario)**\n\nThis funding level breaks the 1 FTE bottleneck by enabling specialized contractor support, allowing work to progress on multiple deliverables simultaneously while maintaining the 12 month runway. By bringing external contractors for software development and other specialized tasks, the lead developer can focus on improving core content quality. With funding to fairly compensate translators, designers, and facilitators, we can move beyond volunteer-dependent work to establish reliable timelines and better demonstrate the project\'s potential with proper support structures.\n\n**Time Allocation & Deliverables (in addition to previous)**\n\n* Core Content Updates  \n   * Write second new chapter: Cybersecurity for AI\n* Video Content Development  \n   * 6-8 videos  \n   * Professionally edited YouTube-style videos instead of simple recorded lectures\n* Translation Program  \n   * Complete professional Chinese translation\n* Facilitation Program  \n   * One complete course round (of multiple cohorts) with fairly compensated facilitators\n* Technical Infrastructure  \n   * Basic software development on automated distillation assistant\n\n**Recurring Costs (75.7%): €8.5k/month (€102.1k total)**\n\n**Contractor Support (24.3%): €32.4k**\n\n**Safety Buffer (10%): €13.4k (10.0%)**\n\n## **€183,000/12 months (Ideal Scenario)**\n\nThis ideal funding enables comprehensive expansion across all deliverables, with greater language coverage, more facilitation rounds, and complete content development across all priority topics. With sufficient specialist support for development, translations, and teaching, we can fully deliver on all six core deliverables without relying on volunteer work. We can fulfill facilitator requests, produce professionally edited videos, guarantee translations in multiple languages, and expand our impact through additional facilitation rounds throughout the year.\n\n**Time Allocation & Deliverables (in addition to previous)**\n\n* Core Content Updates  \n   * Write third new chapter: Agent Foundations\n* Video Content Development  \n   * 9+ professional-quality educational videos\n* Translation Program  \n   * Three languages total (Chinese plus two additional languages)\n* Facilitation Program  \n   * Three facilitation rounds throughout the year  \n   * Printing and distribution of physical materials\n* Technical Infrastructure  \n   * Extensive software development on automated distillation assistant\n\n**Recurring Costs (64.0%): €9.9k/month (€118.7k total)**\n\n**Contractor Support (26.0%): €46.3k**\n\n**Safety Buffer (10%): €17.8k (10.0%)**\n\n## **Scale & sustainability**\n\n**Is there value in additional funding beyond the requested level?** Yes! We are trying to be pragmatic by offering the different tiers with concrete deliverables, but this is a very ambitious project with a lot of scope for development. Additional funding beyond the requested amounts can be used to expand and offer more facilitation programs. We could accommodate more students, or work with facilitators for longer periods to improve course delivery. We could invest in professional-grade video production and editing to create higher quality educational content. With sufficient development resources and several people working together on the project, we could build out a full-fledged educational platform, potentially integrating with established providers like Coursera or Udemy to dramatically increase reach. Finally, we could probably hire more developers to properly extend the alignment research dataset and create a fully fledged AI safety research distillation assistant to make sure the project is sustainable. Each of these expansions directly multiplies our impact by making the content more accessible and engaging to a wider audience.\n\nWe\'re intentionally structured as a non-profit academic initiative rather than seeking self-sustainability through revenue.\n\n## **Reference Classes**\n\nTo ensure our funding request is appropriately calibrated, we\'ve benchmarked our costs against comparable projects and roles in the AI safety space. The minimal scenario is on the lower end of the location adjusted range (€60k), and the ideal scenario is in the middle of the location adjusted range (€80k). The work encompasses technical research, communication, full stack software development, content creation, website design, and video production. The compensation adjusted for experience and work responsibilities seem fair relative to comparable positions:\n\n* [BlueDot AI Safety Specialist (2025, Remote)](https://bluedot.org/ai-safety-specialist/): £60-90k \\~= €72.5-108k\n* [EpochAI Communications Lead (2025, Remote)](https://careers.epoch.ai/postings/77de06b3-2734-480f-8a24-4a6f5d3604d5): $80-110k \\~= €76-105k\n* [AI Digest Technical Staff (2025, Remote)](https://sage-future.org/jobs/mts-ai-digest): $100-200k \\~= €95k-190k\n* MIRI Communications Lead (2024, Remote): $75-175k \\~= €71-165k\n* FAR AI Technical Communications (2024, Remote): $60-100k \\~= €57-95k\n* Rational Animations Scriptwriter (2024, Remote): $50/hour or €60k/y\n* Range: €74k-146k\n* Location adjusted range (- 25%): €55.5k - €109.5k\n\nFacilitator reference classes:\n\n* [CAIS IMLS](https://www.aisafetybook.com/facilitate): $1000/cohort (€950/cohort)\n* [BlueDot AISF](https://aisafetyfundamentals.com/facilitate/): £1180/cohort (€1400/cohort), £27-40/hour (€32-47/hour)\n* Our rate: €12k/cohort (5 facilitators, 10 weeks, 5 hours/week at €25/hour) (each additional cohort by same facilitator paid less due to overlapping work)\n\n# **Premortem**\n\n**Field evolution speed:** AI safety is advancing incredibly rapidly. New papers, results, and paradigm shifts emerge weekly. Our content could become outdated faster than we can update it manually. This is a big risk. This is also why we\'re investing some portion of our time in developing an automated research distillation pipeline - to help us efficiently process and integrate new developments. If successful, this would allow our explanations to scale alongside field developments. This is a core component of not just AI safety distillation and communication work but for automating parts of AI safety research, which itself is a part of the alignment plan in general. If the automated assistance proves insufficient, we would need to significantly narrow our scope to maintain quality. Alternatively, we would need to request more funding for researchers and editors to parallelize the work manually.\n\n**Medium mismatch:** We\'re investing in text-based explanations, but this might not match how people best learn these concepts. While we\'re exploring multiple formats (video, audio, interactive elements), spreading resources across these could prevent any single medium from reaching excellence. Our staged approach lets us test each format with real users before significant investment. If we discover text isn\'t the optimal medium, we can pivot our focus based on user engagement data.\n\n**User experience barriers:** Even excellent content won\'t have an impact if the reading experience is poor. Our website must compete for attention with professionally designed platforms. If navigation is confusing or content presentation is subpar, we\'ll lose readers before they engage with the substance. While we have plans for UI/UX improvements, we need to carefully balance technical development with content quality.\n\n**Scope vs quality tradeoff:** We risk trying to do too many things at once - comprehensive textbook, video series, teaching platform, translation program. Basically falling prey to scope creep and competing on too many fronts. This could lead to delivering mediocre quality across all fronts rather than excellence in key areas. Getting this balance wrong could severely limit our impact as users won\'t return to or recommend a subpar resource. If we try to do everything, we risk doing nothing well enough to matter. We aim to use clear metrics to identify when to double down on what\'s working versus when to explore new approaches.\n\nThanks a lot for reading!
"""

def make_screen_grant_prompt(grant_description: str):
    return f"""
You are tasked with evaluating grant proposals to determine how promising they are to help the transition to powerful AI go well (e.g. to help avoid existential risk from AI). Only evaluate how they project may help make the transition to powerful AI go well, ignore other possible impacts. For each grant proposal you review, please provide a structured evaluation using the following XML format. Use the example below as a reference for your approach and level of analysis.

```xml
<grant_evaluation>
  <project_summary>
    <what_proposed>
      <!-- Provide a concise summary of what the project is proposing to do in its own terms -->
    </what_proposed>
  </project_summary>

  <impact_analysis>
    <possible_impact id="1">
      <impact_statement>
        <!-- Describe the first possible impact of the project on AI going well. Ignore other possible impacts.-->
      </impact_statement>
      <pathway_to_impact>
        <!-- Explain the causal pathway through which this impact might occur -->
      </pathway_to_impact>
      <barriers_to_impact>
        <!-- List specific barriers that might prevent this impact from occurring -->
        <barrier>Barrier 1</barrier>
        <barrier>Barrier 2</barrier>
        <!-- Add more barriers as needed -->
      </barriers_to_impact>
      <further_research>
        <!-- What research would help better understand the likelihood of this impact? -->
        <research_item>Research item 1</research_item>
        <!-- Add more research items as needed -->
      </further_research>
      <likelihood_assessment>
        <!-- Assess how likely this impact is to occur, with reasoning -->
      </likelihood_assessment>
    </possible_impact>

    <possible_impact id="2">
      <!-- Follow the same structure as above for additional possible impacts -->
    </possible_impact>
    
    <!-- Add more possible impacts as appropriate -->
  </impact_analysis>

  <overall_assessment>
    <promise_description>
      <!-- Describe overall how promising this grant is to help make the transition to powerful AI go well -->
    </promise_description>
    <promise_score>
      <!-- Provide a numerical score out of 10 (e.g., 5) with brief justification -->
    </promise_score>
  </overall_assessment>
</grant_evaluation>
```

## Example Evaluation

Here is an example grant description:

{EXAMPLE_GRANT_DESCRIPTION}

And an example analysis:

```xml
<grant_evaluation>
  <project_summary>
    <what_proposed>
      This project aims to address a knowledge transfer bottleneck in the AI safety field by creating comprehensive, high-quality explanations of complex AI safety concepts. The team plans to update their existing 600+ page literature review that maps connections between AI safety problems and solutions, while expanding into new formats including audio recordings, video explanations, and professional translations (prioritizing Chinese). They also intend to develop better teaching materials for facilitators, improve their website, and create new chapters on topics like Cooperative AI.
    </what_proposed>
  </project_summary>

  <impact_analysis>
    <possible_impact id="1">
      <impact_statement>
        More researchers do cutting-edge AI safety research (technical and governance)
      </impact_statement>
      <pathway_to_impact>
        The resources produced by this project help budding AI safety researchers orient towards the field and start to think about how to contribute. Most of these folks will never contribute anything useful, but a few of them may eventually contribute something useful.
      </pathway_to_impact>
      <barriers_to_impact>
        <barrier>These resources may not be a valuable addition to the other resources already available (some of the other resources are mentioned in the proposal)</barrier>
        <barrier>There may be very few budding AI safety researchers who ever actually contribute anything useful</barrier>
      </barriers_to_impact>
      <further_research>
        <research_item>I would review the AI Safety Atlas materials on their website, especially relative to other similar materials</research_item>
      </further_research>
      <likelihood_assessment>
        It's plausible that this impact will occur, but the barriers make it seem fairly unlikely.
      </likelihood_assessment>
    </possible_impact>

    <possible_impact id="2">
      <impact_statement>
        The AI Safety Atlas informs policy-making conversations, leading to better safety policy
      </impact_statement>
      <pathway_to_impact>
        The Atlas becomes a key reference for policymakers
      </pathway_to_impact>
      <barriers_to_impact>
        <barrier>These resources may not add much over policymakers' current understanding, perhaps because these resources are highly technical and not policy-relevant</barrier>
        <barrier>These resources may not be a valuable addition to the other resources already available (some of the other resources are mentioned in the proposal)</barrier>
      </barriers_to_impact>
      <further_research>
        <research_item>I would review the AI Safety Atlas materials on their website, especially relative to other similar materials</research_item>
      </further_research>
      <likelihood_assessment>
        Fairly unlikely, unlikely but plausible that it's both the case that resources like these would help policymakers and also that the AI Safety Atlas will be the best such resource.
      </likelihood_assessment>
    </possible_impact>
  </impact_analysis>

  <overall_assessment>
    <promise_description>
      There's some chance that this grant could be highly impactful for AI safety by causing more researchers do cutting-edge AI safety research or informing policy-making conversations, leading to better safety policy. However, there are many barriers to this and it seems somewhat unlikely.
    </promise_description>
    <promise_score>
      5
    </promise_score>
  </overall_assessment>
</grant_evaluation>
```

When evaluating a grant proposal:

1. First, carefully read and understand the proposal in its entirety
2. Summarize what the project is proposing to do using its own terms and framing
3. Identify any possible impacts the project might have
4. For each possible impact:
   - Describe a clear causal pathway through which the impact might occur
   - Identify specific barriers that might prevent this impact
   - Suggest what further research would help assess likelihood
   - Assess the overall likelihood of this impact occurring
5. Synthesize your analysis into an overall assessment of the grant's promise
6. Provide a numerical score (X/10) to quantify how promising you find the grant. ONLY include a single integer here.

Use specific details from the proposal to ground your analysis, and provide reasoned arguments for your assessments.

## Grant to evaluate:

{grant_description}
""".strip()